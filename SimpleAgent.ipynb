{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export AUTOGRAPH_VERBOSITY=0\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# tf.compat.v1.logging.set_verbosity(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-30T01:44:05.133Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext line_profiler\n",
    "import cantera as ct\n",
    "import gym \n",
    "import time\n",
    "import numpy as np \n",
    "from stable_baselines.common.policies import MlpPolicy, CnnLstmPolicy,MlpLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines import ACKTR, PPO2\n",
    "from envs.sim_env import SimEnv\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " def _train_step(self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,\n",
    "                    writer, states=None, cliprange_vf=None):\n",
    "        \"\"\"\n",
    "        Training of PPO2 Algorithm\n",
    "        :param learning_rate: (float) learning rate\n",
    "        :param cliprange: (float) Clipping factor\n",
    "        :param obs: (np.ndarray) The current observation of the environment\n",
    "        :param returns: (np.ndarray) the rewards\n",
    "        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n",
    "        :param actions: (np.ndarray) the actions\n",
    "        :param values: (np.ndarray) the values\n",
    "        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n",
    "        :param update: (int) the current step iteration\n",
    "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
    "        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n",
    "        :return: policy gradient loss, value function loss, policy entropy,\n",
    "                approximation of kl divergence, updated clipping range, training update operation\n",
    "        :param cliprange_vf: (float) Clipping factor for the value function\n",
    "        \"\"\"\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        td_map = {self.train_model.obs_ph: obs, self.action_ph: actions,\n",
    "                  self.advs_ph: advs, self.rewards_ph: returns,\n",
    "                  self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,\n",
    "                  self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}\n",
    "        if states is not None:\n",
    "            td_map[self.train_model.states_ph] = states\n",
    "            td_map[self.train_model.dones_ph] = masks\n",
    "\n",
    "        if cliprange_vf is not None and cliprange_vf >= 0:\n",
    "            td_map[self.clip_range_vf_ph] = cliprange_vf\n",
    "\n",
    "        if states is None:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs + 1\n",
    "        else:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs // self.n_steps + 1\n",
    "\n",
    "        if writer is not None:\n",
    "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
    "            if self.full_tensorboard_log and (1 + update) % 10 == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map, options=run_options, run_metadata=run_metadata)\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % (update * update_fac))\n",
    "            else:\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map)\n",
    "            writer.add_summary(summary, (update * update_fac))\n",
    "        else:\n",
    "            policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)\n",
    "\n",
    "        return policy_loss, value_loss, policy_entropy, approxkl, clipfrac\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Original model__: \n",
    "```python\n",
    "model = PPO2(MlpLstmPolicy, env, gamma=0.99, n_steps=256, ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,\n",
    "                 max_grad_norm=0.5, lam=0.95, nminibatches=1, noptepochs=6, cliprange=0.2, cliprange_vf=None,\n",
    "                 verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None,\n",
    "                 full_tensorboard_log=False) # Original model\n",
    "```               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T01:17:08.242457Z",
     "start_time": "2019-10-30T01:17:02.932751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065c55915d5f4adf9ad351676f3405a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fc4c2427250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fc4c2427250>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fc4c2182f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fc4c2182f90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starting_timestep = 475000\n",
    "step_size = 25000\n",
    "for i in tqdm_notebook(range(0, 1)):\n",
    "    model = PPO2.load(f\"Trained Models/PPO2_MlpLstmPolicy_11072019_{starting_timestep:0.0f}.zip\")\n",
    "    env = SubprocVecEnv([SimEnv])\n",
    "    model.set_env(env)\n",
    "    model.learn(total_timesteps=step_size)\n",
    "\n",
    "    starting_timestep += step_size\n",
    "    model.save(f\"Trained Models/PPO2_MlpLstmPolicy_11072019_{starting_timestep:0.0f}.zip\")\n",
    "    time.sleep(5) # Wait for 5 seconds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# # env = DummyVecEnv([SimEnv])\n",
    "# env = SimEnv()\n",
    "# obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# game_over = False\n",
    "# while not game_over: \n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, game_over, info = env.step(action)\n",
    "#     env.render(mode='human')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
