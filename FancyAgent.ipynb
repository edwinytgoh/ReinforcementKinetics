{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "import cantera as ct\n",
    "import os\n",
    "import gym \n",
    "import numpy as np \n",
    "from stable_baselines.common.policies import MlpPolicy, CnnLstmPolicy,MlpLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines import ACKTR\n",
    "from envs.sim_env import SimEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(rank, log_dir, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = Monitor(SimEnv(), log_dir)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_global_seeds(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "  \"\"\"\n",
    "  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "  global n_steps, best_mean_reward\n",
    "  # Print stats every 1000 calls\n",
    "  if (n_steps + 1) % 1000 == 0:\n",
    "      # Evaluate policy training performance\n",
    "      x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "      if len(x) > 0:\n",
    "          mean_reward = np.mean(y[-100:])\n",
    "          print(x[-1], 'timesteps')\n",
    "          print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "          # New best model, you could save the agent here\n",
    "          if mean_reward > best_mean_reward:\n",
    "              best_mean_reward = mean_reward\n",
    "              # Example for saving best model\n",
    "              print(\"Saving new best model\")\n",
    "              _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "  n_steps += 1\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = 8  # Number of processes to use\n",
    "\n",
    "# Create log dir\n",
    "log_dirs = [f\"/tmp/gym/{i}\" for i in range(num_cpu)]\n",
    "[os.makedirs(log_dir, exist_ok=True) for log_dir in log_dirs]\n",
    "# Create the vectorized environment\n",
    "env = SubprocVecEnv([make_env(i, log_dirs[i]) for i in range(num_cpu)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
